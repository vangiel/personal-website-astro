---
title: "Basics of Machine Learning, the Multi-Layer Perceptron"
pubDate: "Jul 08 2022"
heroImage: "@/images/blog/post1/neuron_perceptron.png"
imageAlt: "Diagram of a neuron and a perceptron"
tags: ["AI", "Basic concept"]
---

import neuronImage from "@/images/blog/post1/neuron_perceptron.png";
import { Image } from "astro:assets";

This first section will guide the reader through the basic concept of DNNs using the Multi-Layer Perceptron (MLP) as the explanation baseline. MLP's simplicity makes it easier to understand the structure and elementary processes within Artificial Neural Networks (ANN) and the definitions here can be extrapolated to more complex DNNs. This section does not intend to provide an exhaustive explanation of the training and optimisation of processes but to introduce the key ideas that will help understand the rest of the thesis.

The **Multi-layer Perceptron (MLP)** was one of the first ANNs to appear in the machine learning community. It is an extension of the Perceptron developed by Rosenblatt in 1958 **citation here**. This kind of network, in theory, can ``learn'' to approximate any kind of linear or non-linear functions. Thereupon, we will dip into the MLP structure to understand how it computes its outputs.

<Image src={neuronImage} alt="Comparison of a biological neuron and an artificial neuron" />

The building block of the MLP is the **Perceptron** referred to as an artificial neuron.
It is inspired by biological neurons, sharing some similarities as depicted in Figure **reference here**. It aggregates information from different inputs (dendrites) and generates a single output (axon) that is triggered using an activation function.

$$a = \sigma (\sum_\{i\} w_\{i\}x_\{i\} + b)$$
